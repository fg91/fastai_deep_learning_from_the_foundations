{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='46' class='' max='46', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [46/46 00:27<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok, proc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n",
    "pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\n",
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWD-LSTM\n",
    "![](pictures/lstm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take advantage of the GPU we do one matrix multiplication and split the output into 4 chunks for the 4 gates instead of doing 4 matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        self.ih = nn.Linear(ni, 4*nh)\n",
    "        self.hh = nn.Linear(nh, 4*nh)\n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        h, c = state  # (64, 300) each\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)  # (64, 1200) -> 4x * (64, 300) because args of chunk are (chunks, dim)\n",
    "        \n",
    "        ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "        \n",
    "        c = (forgetgate * c) + (ingate * cellgate)  # (64, 300)\n",
    "        h = outgate * c.tanh()                      # (64, 300)\n",
    "        \n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = LSTMCell(*cell_args)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        # input (64, 70, 300), state both (64, 300)\n",
    "        inputs = input.unbind(1)  # (64, 70, 300) -> tuple of len 70 with shape (64, 300)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs, dim=1), state  # (64, 70, 300) and 2 tuple of shapes (64, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(LSTMCell, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (torch.zeros(64, 300), torch.zeros(64, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.8 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y, h1 = lstm(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_fn(f):\n",
    "    f()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.4 ms ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builtin LSTM version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(300, 300, 1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)\n",
    "h = (torch.zeros(1, 64, 300), torch.zeros(1, 64, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.5 ms ± 42.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y, h1 = lstm(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78 ms ± 17 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU versions almost has the same speed. On the GPU, however, PyTorch uses CuDNN behind the scenes which greatly optimized the for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit as jit\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(jit.ScriptModule):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))\n",
    "        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))\n",
    "        self.bias_ih = nn.Parameter(torch.randn(4 * nh))\n",
    "        self.bias_hh = nn.Parameter(torch.randn(4 * nh))\n",
    "\n",
    "    @jit.script_method\n",
    "    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        hx, cx = state\n",
    "        gates = (input @ self.w_ih.t() + self.bias_ih +\n",
    "                 hx @ self.w_hh.t() + self.bias_hh)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(jit.ScriptModule):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "\n",
    "    @jit.script_method\n",
    "    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        inputs = input.unbind(1)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(LSTMCell, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)\n",
    "h = (torch.zeros(64, 300),torch.zeros(64, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.6 ms ± 3.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y,h1 = lstm(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm,x,h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.72 ms ± 365 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU version again has the same speed but with the jit version we almost have the same speed from scratch as CuDNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "#### RNN Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights that are not nullified are corrected by a factor of 1-p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 2., 2., 2., 2., 2., 2., 2., 0.],\n",
       "        [2., 2., 0., 2., 0., 2., 0., 2., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [2., 2., 2., 2., 2., 0., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 2., 0., 0., 2., 0., 2.],\n",
       "        [0., 0., 0., 0., 2., 0., 0., 2., 2., 2.],\n",
       "        [2., 0., 2., 2., 2., 0., 2., 0., 0., 2.],\n",
       "        [2., 0., 0., 2., 2., 2., 2., 0., 0., 0.],\n",
       "        [0., 0., 0., 2., 2., 2., 2., 0., 0., 0.],\n",
       "        [2., 2., 2., 2., 0., 2., 0., 0., 2., 2.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = dropout_mask(x, (10, 10), 0.5); mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the dropout mask is simply done by `x * mask`.\n",
    "Why don't we use PyTorch's dropout? We do not want to nullify all the coefficients randomly: on the sequence dimension we want to always nullify the same positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0600)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()/mask.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4572), tensor(1.0610))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*mask).std(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the RNN, the tensors have the shape (bs, seq_len, vocab_size). We want to apply the dropout mask across the seq_len dimension, we therefore create a dropout mask for the first and third demension and broadcast it along the seq_len dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        \n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.9303,  1.6430, -0.9806, -0.1865, -0.7762,  0.2402,  0.1565],\n",
       "          [ 1.9869,  1.0205,  1.0020,  0.1264, -0.0188,  0.7346,  0.5531],\n",
       "          [ 1.6017, -0.4937,  0.3259, -0.4519, -0.6995,  1.1109,  0.0447]],\n",
       " \n",
       "         [[-0.7217, -0.0594,  0.5015,  0.3515,  0.8265, -0.8099,  0.2417],\n",
       "          [ 0.0783,  1.6686,  0.5927, -1.3416,  0.1471, -0.9153,  1.3977],\n",
       "          [-0.3767, -0.0497,  0.5111, -1.8193,  0.8391, -0.2002,  0.2831]],\n",
       " \n",
       "         [[-1.2901, -1.2654,  0.4532, -0.9002,  0.2530, -0.7915,  0.5956],\n",
       "          [-0.2278, -2.3555, -0.1417,  0.6592, -0.6649,  2.1027, -1.4821],\n",
       "          [-1.2690,  0.9549, -0.4220,  0.1389, -1.2230,  0.1971,  2.0048]]]),\n",
       " tensor([[[-1.3290,  2.3472, -1.4009, -0.0000, -1.1089,  0.3432,  0.0000],\n",
       "          [-0.0000, -0.0848,  0.0000,  0.5021,  1.1807, -1.1570,  0.0000],\n",
       "          [-1.8430, -0.0000,  0.6474, -1.2859,  0.0000, -0.0000,  0.8509]],\n",
       " \n",
       "         [[ 2.8384,  1.4578,  1.4315,  0.0000, -0.0269,  1.0495,  0.0000],\n",
       "          [ 0.0000,  2.3838,  0.0000, -1.9165,  0.2101, -1.3075,  0.0000],\n",
       "          [-0.3255, -0.0000, -0.2024,  0.9417, -0.0000,  0.0000, -2.1173]],\n",
       " \n",
       "         [[ 2.2882, -0.7053,  0.4656, -0.0000, -0.9992,  1.5871,  0.0000],\n",
       "          [-0.0000, -0.0710,  0.0000, -2.5991,  1.1987, -0.2860,  0.0000],\n",
       "          [-1.8128,  0.0000, -0.6028,  0.1984, -0.0000,  0.0000,  2.8641]]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = RNNDropout(0.3)\n",
    "test_input = torch.randn(3, 3, 7)\n",
    "test_input, dp(test_input).transpose(0,1)  \n",
    "# transpose to make the seq_len dim come first and visualize that always same fields are nullified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Dropout\n",
    "Weight Dropout is applied to the weights of the inner LSTM hidden to hidden matrix. Hacky if we want to preserve the CuDNN speed and not reimplement cell from scracth. We add parameter that will contain the raw weights and we replace the weight matrix in the LSTM at the beginnning of each forward pass.\n",
    "\n",
    "[Use this instead?](https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/weight_drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = nn.LSTM(5, 2)  # input_sz, hid_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_module = WeightDropout(module, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before applying module:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2782,  0.5664],\n",
       "        [-0.3514,  0.2104],\n",
       "        [ 0.3407,  0.4183],\n",
       "        [-0.5449,  0.1244],\n",
       "        [ 0.6391, -0.3578],\n",
       "        [ 0.5812,  0.1719],\n",
       "        [ 0.2902, -0.2362],\n",
       "        [-0.1949, -0.4394]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2782,  0.5664],\n",
       "        [-0.3514,  0.2104],\n",
       "        [ 0.3407,  0.4183],\n",
       "        [-0.5449,  0.1244],\n",
       "        [ 0.6391, -0.3578],\n",
       "        [ 0.5812,  0.1719],\n",
       "        [ 0.2902, -0.2362],\n",
       "        [-0.1949, -0.4394]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module, f'{WEIGHT_HH}_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After applying module:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(4, 20, 5)  # bs, seq_len, hid_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (torch.zeros(1, 20, 2), torch.zeros(1, 20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, h = dp_module(test_input, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.5664],\n",
       "        [-0.3514,  0.2104],\n",
       "        [ 0.0000,  0.4183],\n",
       "        [-0.5449,  0.0000],\n",
       "        [ 0.6391, -0.3578],\n",
       "        [ 0.5812,  0.0000],\n",
       "        [ 0.2902, -0.2362],\n",
       "        [-0.0000, -0.0000]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, WEIGHT_HH) * (1 - 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, h = dp_module(test_input, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2782,  0.5664],\n",
       "        [-0.0000,  0.2104],\n",
       "        [ 0.3407,  0.4183],\n",
       "        [-0.5449,  0.0000],\n",
       "        [ 0.0000, -0.3578],\n",
       "        [ 0.5812,  0.1719],\n",
       "        [ 0.0000, -0.0000],\n",
       "        [-0.1949, -0.4394]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, WEIGHT_HH) * (1 - 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the `dp_module` is called different fields of the weight matrix are nullified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original is unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2782,  0.5664],\n",
       "        [-0.3514,  0.2104],\n",
       "        [ 0.3407,  0.4183],\n",
       "        [-0.5449,  0.1244],\n",
       "        [ 0.6391, -0.3578],\n",
       "        [ 0.5812,  0.1719],\n",
       "        [ 0.2902, -0.2362],\n",
       "        [-0.1949, -0.4394]], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module, f'{WEIGHT_HH}_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Dropout\n",
    "Applies dropout to full rows of the embedding matrix (zeroes embedding for specific words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb, self.embed_p = emb, embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "            \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0), 1)  # (100, 1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            # list of 100 numbers being eiter 0 or 2\n",
    "            masked_embed = self.emb.weight * mask  # some words are zeroed\n",
    "        \n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        \n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Docstring:\n",
    "A simple lookup table that looks up embeddings in a fixed dictionary and size.\n",
    "\n",
    "This module is often used to retrieve word embeddings using indices.\n",
    "The input to the module is a list of indices, and the embedding matrix,\n",
    "and the output is the corresponding word embeddings.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)  # 100 embeddings of size 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dp = EmbeddingDropout(enc, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randint(0, 100, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55, 60, 97])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3140, -0.8487,  2.2809,  0.3752,  1.5074, -1.0890, -0.2587],\n",
       "        [-0.1018, -4.3376,  2.3576, -0.3561, -1.9399,  1.7028, -0.2791],\n",
       "        [ 0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dp(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeroes embedding for specific words (emb_sz = 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWD-LSTM Main Model\n",
    "Regular multilayer LSTM with all those kinds of dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_detach(h):\n",
    "    \"Detaches h from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "class AWD_LSTM(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs, self.emb_sz, self.n_hid, self.n_layers = 1, emb_sz, n_hid, n_layers  # bs = 1 to make it reset h\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(input_size=emb_sz if l == 0 else n_hid, \n",
    "                             hidden_size=(n_hid if l != n_layers - 1 else emb_sz),\n",
    "                             num_layers=1, batch_first=True\n",
    "                            ) for l in range(n_layers)]\n",
    "        \"\"\"\n",
    "        (Pdb) self.rnns[0]\n",
    "        LSTM(300, 500, batch_first=True)\n",
    "        (Pdb) self.rnns[1]\n",
    "        LSTM(500, 500, batch_first=True)\n",
    "        (Pdb) self.rnns[2]\n",
    "        LSTM(500, 300, batch_first=True)\n",
    "        (Pdb) self.rnns[3]\n",
    "        \"\"\"\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        # Initialize embedding\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "        if bs!=self.bs:  # self.bs = 1 at beginning, forces to reset h\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "            \n",
    "        # self.hidden ist list of len 3 of tuples of len 2 of tensors of shape [1, 64, 500/300]\n",
    "        # input shape [64, 70]\n",
    "        raw_output = self.input_dp(self.emb_dp(input))  # [64, 70, 300], dropout on input along seq dimension\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            # new_h has shape [1, 64, 500] for l = 0 (final hidden state), raw_output [64, 70, 500] for l == 0\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)  # RNNDropout along seq dim on h's\n",
    "            outputs.append(raw_output) \n",
    "        # here new_hidden is list of len n_layers with tuples of two tensors h,c each\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs  # with and without dropout\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randint(low=0, high=30000, size=(64, 70)) # (bs, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm = AWD_LSTM(30000, 300, 500, 3, vocab.index(PAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs, outputs = awd_lstm(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_outputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70, 300]), torch.Size([64, 70, 300]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_outputs[-1].shape, outputs[-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the first layers dropout is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_outputs[0] == outputs[0]).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the last layer *no* dropout is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_outputs[-1] == outputs[-1]).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        # raw_outputs[-1] is equal to outputs[-1]\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))  # (64 * 70, 300) -> (64 * 70, voc_size)\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_dec = LinearDecoder(len(vocab), 300, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lin_dec((raw_outputs, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4480, 60002]), 4480)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0].shape, 64*70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack all of it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token, \n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pad = vocab.index(PAD); tok_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = get_language_model(len(vocab), 300, 500, 2, tok_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_model(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded, raw_outputs, outputs = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4480, 60002])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_outputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([64, 70, 500]), torch.Size([64, 70, 300])],\n",
       " [torch.Size([64, 70, 500]), torch.Size([64, 70, 300])])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.size() for o in raw_outputs], [o.size() for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((raw_outputs[0] == outputs[0]).all().item(), \n",
    " (raw_outputs[-1] == outputs[-1]).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three things are returned to help with regularization:\n",
    "\n",
    "* The true output, probabilities for each word\n",
    "* Activations of encoder with and without dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip gradients to allow larger learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None): self.clip = clip\n",
    "    def after_backward(self):\n",
    "        if self.clip:\n",
    "            nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a `RNNTrainer` Callback to do the following things:\n",
    "    \n",
    "* Change the output to make it contain only the decoded tensor (for loss) and store `raw_outputs` and `outputs`\n",
    "* Apply Activation Regularization (AR): add a l2 penalty on the last activattions of the AWD LSTM (with dropout applied)\n",
    "* Apply Temoral Activation Regularization (TAR): add a l2 penalty on the difference between two consecutive (in terms of words) raw outputs\n",
    "* Trigger the shuffle of the LMDataset at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNTrainer(Callback):\n",
    "    def __init__(self, alpha, beta):\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "        \n",
    "    def after_pred(self):\n",
    "        \"\"\"Return true prediction, save encoder outputs\"\"\"\n",
    "        self.raw_out, self.out = self.pred[1], self.pred[2]\n",
    "        self.run.pred = self.pred[0]\n",
    "        \n",
    "    def after_loss(self):\n",
    "        # AR and TAR\n",
    "        if self.alpha != 0:\n",
    "            self.run.loss += self.alpha * self.out[-1].float().pow(2).mean()\n",
    "            # out[-1] has shape [64, 70, 300]\n",
    "        if self.beta != 0:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h) > 1:  # should this rather be h.size(1) and not bs?\n",
    "                self.run.loss += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "                \n",
    "    def begin_epoch(self):\n",
    "        # Shuffle the texts at the beginning of the epoch\n",
    "        if hasattr(self.dl.dataset, \"batchify\"): self.dl.dataset.batchify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and metric\n",
    "Implement a flattened version of cross entropy and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_entropy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))\n",
    "\n",
    "def accuracy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return accuracy(input.view(bs * sl, -1), target.view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback, accuracy_flat),\n",
    "       CudaCallback, Recorder,\n",
    "       partial(GradientClipping, clip=0.),\n",
    "       partial(RNNTrainer, alpha=2., beta=1.),\n",
    "       ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(test_model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_flat</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_flat</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.098646</td>\n",
       "      <td>0.200743</td>\n",
       "      <td>4.558224</td>\n",
       "      <td>0.245648</td>\n",
       "      <td>14:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12a_awd_lstm.ipynb to exp/nb_12a.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 12a_awd_lstm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastaiV1",
   "language": "python",
   "name": "fastaiv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
