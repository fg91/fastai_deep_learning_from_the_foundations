{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='46' class='' max='46', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [46/46 00:27<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok, proc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n",
    "pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\n",
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWD-LSTM\n",
    "![](pictures/lstm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take advantage of the GPU we do one matrix multiplication and split the output into 4 chunks for the 4 gates instead of doing 4 matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        self.ih = nn.Linear(ni, 4*nh)\n",
    "        self.hh = nn.Linear(nh, 4*nh)\n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        h, c = state  # (64, 300) each\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1)  # (64, 1200) -> 4x * (64, 300) because args of chunk are (chunks, dim)\n",
    "        \n",
    "        ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "        \n",
    "        c = (forgetgate * c) + (ingate * cellgate)  # (64, 300)\n",
    "        h = outgate * c.tanh()                      # (64, 300)\n",
    "        \n",
    "        return h, (h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = LSTMCell(*cell_args)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        # input (64, 70, 300), state both (64, 300)\n",
    "        inputs = input.unbind(1)  # (64, 70, 300) -> tuple of len 70 with shape (64, 300)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs, dim=1), state  # (64, 70, 300) and 2 tuple of shapes (64, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(LSTMCell, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (torch.zeros(64, 300), torch.zeros(64, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3 ms ± 1.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y, h1 = lstm(x, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_fn(f):\n",
    "    f()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 ms ± 213 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builtin LSTM version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(300, 300, 1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)\n",
    "h = (torch.zeros(1, 64, 300), torch.zeros(1, 64, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.4 ms ± 71.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y, h1 = lstm(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.76 ms ± 19.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU versions almost has the same speed. On the GPU, however, PyTorch uses CuDNN behind the scenes which greatly optimized the for loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit as jit\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(jit.ScriptModule):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.w_ih = nn.Parameter(torch.randn(4 * nh, ni))\n",
    "        self.w_hh = nn.Parameter(torch.randn(4 * nh, nh))\n",
    "        self.bias_ih = nn.Parameter(torch.randn(4 * nh))\n",
    "        self.bias_hh = nn.Parameter(torch.randn(4 * nh))\n",
    "\n",
    "    @jit.script_method\n",
    "    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        hx, cx = state\n",
    "        gates = (input @ self.w_ih.t() + self.bias_ih +\n",
    "                 hx @ self.w_hh.t() + self.bias_hh)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "        hy = outgate * torch.tanh(cy)\n",
    "\n",
    "        return hy, (hy, cy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(jit.ScriptModule):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "\n",
    "    @jit.script_method\n",
    "    def forward(self, input:Tensor, state:Tuple[Tensor, Tensor])->Tuple[Tensor, Tuple[Tensor, Tensor]]:\n",
    "        inputs = input.unbind(1)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs, dim=1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(LSTMCell, 300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 300)\n",
    "h = (torch.zeros(64, 300),torch.zeros(64, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.8 ms ± 3.79 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y,h1 = lstm(x,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm,x,h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.25 ms ± 50.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU version again has the same speed but with the jit version we almost have the same speed from scratch as CuDNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "#### RNN Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights that are not nullified are corrected by a factor of 1-p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 2., 0., 0., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2., 0., 0., 2., 2.],\n",
       "        [2., 0., 2., 0., 2., 2., 0., 0., 2., 0.],\n",
       "        [0., 2., 0., 0., 2., 2., 0., 0., 0., 2.],\n",
       "        [0., 0., 2., 2., 2., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 0., 2., 0., 0., 0., 2., 2., 2.],\n",
       "        [0., 2., 0., 0., 0., 0., 2., 0., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 2., 2., 0., 2., 2.],\n",
       "        [0., 0., 0., 2., 2., 0., 2., 0., 0., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = dropout_mask(x, (10, 10), 0.5); mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the dropout mask is simply done by `x * mask`.\n",
    "Why don't we use PyTorch's dropout? We do not want to nullify all the coefficients randomly: on the sequence dimension we want to always nullify the same positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1200)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()/mask.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6349), tensor(1.0754))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*mask).std(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the RNN, the tensors have the shape (bs, seq_len, vocab_size). We want to apply the dropout mask across the seq_len dimension, we therefore create a dropout mask for the first and third demension and broadcast it along the seq_len dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        \n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.5056,  0.0121, -0.1962, -0.6330, -0.2526,  1.0439, -0.6941],\n",
       "          [ 1.1515,  0.0723,  0.3830,  1.2073, -0.4272, -0.0685, -0.2054],\n",
       "          [ 1.0184, -0.0118, -0.7056,  1.3648, -0.0542, -0.4178, -1.2435]],\n",
       " \n",
       "         [[ 0.5554, -0.6984,  0.4897,  0.5830, -2.5208, -0.0852,  0.5275],\n",
       "          [ 0.1852, -0.3760,  2.3418,  0.1264, -0.2487,  2.6627, -0.7511],\n",
       "          [ 1.2503,  0.9745,  0.0933,  0.4386, -1.1615,  0.6218,  0.1381]],\n",
       " \n",
       "         [[-0.4208,  1.0365,  0.2137,  1.8034,  0.3262,  2.2972,  0.5066],\n",
       "          [-0.3487,  1.6206,  1.1809, -0.1099, -2.6683,  0.6671, -0.6432],\n",
       "          [-0.9166,  1.5202, -0.8253,  1.7873,  1.5201, -1.1465, -0.1496]]]),\n",
       " tensor([[[ 0.7223,  0.0000, -0.2802, -0.9043, -0.3609,  0.0000, -0.0000],\n",
       "          [ 0.0000, -0.9977,  0.0000,  0.8328, -0.0000, -0.1217,  0.7535],\n",
       "          [-0.6011,  0.0000,  0.0000,  2.5762,  0.4660,  3.2818,  0.7237]],\n",
       " \n",
       "         [[ 1.6449,  0.0000,  0.5472,  1.7247, -0.6102, -0.0000, -0.0000],\n",
       "          [ 0.0000, -0.5371,  0.0000,  0.1806, -0.0000,  3.8038, -1.0730],\n",
       "          [-0.4981,  0.0000,  0.0000, -0.1570, -3.8118,  0.9530, -0.9188]],\n",
       " \n",
       "         [[ 1.4548, -0.0000, -1.0080,  1.9498, -0.0774, -0.0000, -0.0000],\n",
       "          [ 0.0000,  1.3921,  0.0000,  0.6265, -0.0000,  0.8883,  0.1973],\n",
       "          [-1.3095,  0.0000, -0.0000,  2.5533,  2.1716, -1.6379, -0.2137]]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = RNNDropout(0.3)\n",
    "test_input = torch.randn(3, 3, 7)\n",
    "test_input, dp(test_input).transpose(0,1)  \n",
    "# transpose to make the seq_len dim come first and visualize that always same fields are nullified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Dropout\n",
    "Weight Dropout is applied to the weights of the inner LSTM hidden to hidden matrix. Hacky if we want to preserve the CuDNN speed and not reimplement cell from scracth. We add parameter that will contain the raw weights and we replace the weight matrix in the LSTM at the beginnning of each forward pass.\n",
    "\n",
    "[Use this instead?](https://pytorchnlp.readthedocs.io/en/latest/_modules/torchnlp/nn/weight_drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = nn.LSTM(5, 2)  # input_sz, hid_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_module = WeightDropout(module, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before applying module:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0058, -0.1290],\n",
       "        [ 0.0350, -0.3804],\n",
       "        [ 0.5237,  0.5607],\n",
       "        [ 0.5398,  0.4805],\n",
       "        [ 0.1579,  0.6691],\n",
       "        [-0.1472,  0.0067],\n",
       "        [ 0.1549, -0.2744],\n",
       "        [-0.2423,  0.3669]], requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0058, -0.1290],\n",
       "        [ 0.0350, -0.3804],\n",
       "        [ 0.5237,  0.5607],\n",
       "        [ 0.5398,  0.4805],\n",
       "        [ 0.1579,  0.6691],\n",
       "        [-0.1472,  0.0067],\n",
       "        [ 0.1549, -0.2744],\n",
       "        [-0.2423,  0.3669]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module, f'{WEIGHT_HH}_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After applying module:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(4, 20, 5)  # bs, seq_len, hid_sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = (torch.zeros(1, 20, 2), torch.zeros(1, 20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, h = dp_module(test_input, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.1290],\n",
       "        [ 0.0000, -0.3804],\n",
       "        [ 0.5237,  0.5607],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [ 0.1579,  0.0000],\n",
       "        [-0.1472,  0.0067],\n",
       "        [ 0.0000, -0.2744],\n",
       "        [-0.2423,  0.3669]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, WEIGHT_HH) * (1 - 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, h = dp_module(test_input, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0058, -0.1290],\n",
       "        [ 0.0000, -0.3804],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [ 0.0000,  0.4805],\n",
       "        [ 0.0000,  0.0000],\n",
       "        [-0.1472,  0.0000],\n",
       "        [ 0.1549, -0.0000],\n",
       "        [-0.2423,  0.3669]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module.module, WEIGHT_HH) * (1 - 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time the `dp_module` is called different fields of the weight matrix are nullified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original is unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0058, -0.1290],\n",
       "        [ 0.0350, -0.3804],\n",
       "        [ 0.5237,  0.5607],\n",
       "        [ 0.5398,  0.4805],\n",
       "        [ 0.1579,  0.6691],\n",
       "        [-0.1472,  0.0067],\n",
       "        [ 0.1549, -0.2744],\n",
       "        [-0.2423,  0.3669]], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(dp_module, f'{WEIGHT_HH}_raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Dropout\n",
    "Applies dropout to full rows of the embedding matrix (zeroes embedding for specific words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb, self.embed_p = emb, embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "            \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0), 1)  # (100, 1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            # list of 100 numbers being eiter 0 or 2\n",
    "            masked_embed = self.emb.weight * mask  # some words are zeroed\n",
    "        \n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        \n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Docstring:\n",
    "A simple lookup table that looks up embeddings in a fixed dictionary and size.\n",
    "\n",
    "This module is often used to retrieve word embeddings using indices.\n",
    "The input to the module is a list of indices, and the embedding matrix,\n",
    "and the output is the corresponding word embeddings.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)  # 100 embeddings of size 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dp = EmbeddingDropout(enc, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randint(0, 100, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80, 74, 77])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7836,  1.6946,  3.9820,  2.2164, -3.0690, -3.9632, -2.7137],\n",
       "        [-2.4584,  2.6614,  3.5521, -2.1865,  2.8898, -1.4482,  3.0444],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_dp(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeroes embedding for specific words (emb_sz = 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWD-LSTM Main Model\n",
    "Regular multilayer LSTM with all those kinds of dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_detach(h):\n",
    "    \"Detaches h from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#export\n",
    "class AWD_LSTM(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs, self.emb_sz, self.n_hid, self.n_layers = 1, emb_sz, n_hid, n_layers  # bs = 1 to make it reset h\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(input_size=emb_sz if l == 0 else n_hid, \n",
    "                             hidden_size=(n_hid if l != n_layers - 1 else emb_sz),\n",
    "                             num_layers=1, batch_first=True\n",
    "                            ) for l in range(n_layers)]\n",
    "        \"\"\"\n",
    "        (Pdb) self.rnns[0]\n",
    "        LSTM(300, 500, batch_first=True)\n",
    "        (Pdb) self.rnns[1]\n",
    "        LSTM(500, 500, batch_first=True)\n",
    "        (Pdb) self.rnns[2]\n",
    "        LSTM(500, 300, batch_first=True)\n",
    "        (Pdb) self.rnns[3]\n",
    "        \"\"\"\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        # Initialize embedding\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "        if bs!=self.bs:  # self.bs = 1 at beginning, forces to reset h\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "            \n",
    "        # self.hidden ist list of len 3 of tuples of len 2 of tensors of shape [1, 64, 500/300]\n",
    "        # input shape [64, 70]\n",
    "        raw_output = self.input_dp(self.emb_dp(input))  # [64, 70, 300], dropout on input along seq dimension\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            # new_h has shape [1, 64, 500] for l = 0 (final hidden state), raw_output [64, 70, 500] for l == 0\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)  # RNNDropout along seq dim on h's\n",
    "            outputs.append(raw_output) \n",
    "        # here new_hidden is list of len n_layers with tuples of two tensors h,c each\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs  # with and without dropout\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randint(low=0, high=30000, size=(64, 70)) # (bs, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm = AWD_LSTM(30000, 300, 500, 3, vocab.index(PAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs, outputs = awd_lstm(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_outputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70, 500]), torch.Size([64, 70, 500]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_outputs[0].shape, outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastaiV1",
   "language": "python",
   "name": "fastaiv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
