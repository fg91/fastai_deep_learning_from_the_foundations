{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_12a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/fabiograetz/.fastai/data/imdb')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ll.train.proc_x[1].vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuing the LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_pad = vocab.index(PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 300, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_wgts = torch.load(path/'pretrained'/'pretrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_vocab = pickle.load(open(path/'pretrained'/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ids in the vocab depend on the token frequency in the corpus. We have to match the new (IMDB) vocab to the old (WT103) corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_house_new, idx_house_old = vocab.index('house'), old_vocab.index('house')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(348, 231)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_house_new, idx_house_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to match the embedding and the decoder weights accordingly. For tokens that did not exist in the old vocabulary and, thus, in the embedding, we use the mean of the pretrained embedding weights/decoder bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_wgt = old_wgts['0.emb.weight'][idx_house_old]\n",
    "house_bias= old_wgts['1.decoder.bias'][idx_house_old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300]), tensor(-6.5101, device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_wgt.shape, house_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60001, 300]), torch.Size([60001]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_wgts['0.emb.weight'].shape, old_wgts['1.decoder.bias'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    wgts = old_wgts['0.emb.weight']\n",
    "    bias = old_wgts['1.decoder.bias']\n",
    "    wgts_m, bias_m = wgts.mean(dim=0), bias.mean()\n",
    "\n",
    "    new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))\n",
    "    new_bias = bias.new_zeros(len(vocab))\n",
    "    \n",
    "    otoi = {v:k for k,v in enumerate(old_vocab)}\n",
    "    \n",
    "    for i, w in enumerate(new_vocab):\n",
    "        if w in otoi:\n",
    "            idx = otoi[w]  # old idx\n",
    "            new_wgts[i], new_bias[i] = wgts[idx], bias[idx]\n",
    "        else:\n",
    "            new_wgts[i], new_bias[i] = wgts_m, bias_m\n",
    "        \n",
    "    old_wgts['0.emb.weight'] = new_wgts\n",
    "    old_wgts['0.emb_dp.emb.weight'] = new_wgts\n",
    "    old_wgts['1.decoder.weight']    = new_wgts  # weight tying\n",
    "    old_wgts['1.decoder.bias']      = new_bias\n",
    "    \n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = match_embeds(old_wgts, old_vocab, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the token \"house\" was properly converted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(wgts['0.emb.weight'][idx_house_new], house_wgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(wgts['1.decoder.bias'][idx_house_new], house_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained weights into our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(wgts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (emb): Embedding(60002, 300, padding_idx=1)\n",
       "    (emb_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60002, 300, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(300, 300, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(300, 300, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (output_dp): RNNDropout()\n",
       "    (decoder): Linear(in_features=300, out_features=60002, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to apply discriminative learning rates. In order to do so, we have to split our model in different layer groups:\n",
    "\n",
    "We split the rnn/corresponding dropouts into two groups plus one additional group for the embeddings/decoder. The latter has to be trained the most because of the new embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_splitter(m):\n",
    "    groups = []\n",
    "    for i in range(len(m[0].rnns)):\n",
    "        groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n",
    "    groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]\n",
    "\n",
    "    return [list(o.parameters()) for o in groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we freeze the RNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback, accuracy_flat),\n",
    "       CudaCallback, Recorder,\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       partial(RNNTrainer, alpha=2., beta=1.),\n",
    "       ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(), cb_funcs=cbs, splitter=lm_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_1cycle_anneal(start, high, end):\n",
    "    return [sched_cos(start, high), sched_cos(high, end)]\n",
    "\n",
    "def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):\n",
    "    phases = create_phases(pct_start)\n",
    "    sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "                 for lr in lrs]\n",
    "    sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))\n",
    "    return [ParamScheduler('lr', sched_lr),\n",
    "            ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsched = sched_1cycle([lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5gUVdYH4N9hImHIUYIDSpQ4jAMokhRWQDErmF0VAwZ0PxV0xbSuru6a1ggY1lVEXMEASpAsSYaM5DAkgQnEYWAS5/ujq3qqu6u6q7qrQ/Wc93l4pruquurWTHPq1q17zyVmhhBCCOerEu0CCCGEsIcEdCGEiBMS0IUQIk5IQBdCiDghAV0IIeJEYrQOXL9+fU5PT4/W4YUQwpFWrVqVz8wN9NZFLaCnp6cjOzs7WocXQghHIqI9RuukyUUIIeKEBHQhhIgTEtCFECJOSEAXQog4IQFdCCHihAR0IYSIExLQhRAiTjguoG89dBJvzN6K/MLiaBdFCCFiiuMC+vbck3hn3g4cOVUS7aIIIURMMTVSlIhyAJwEUA6gjJkzdbbpB+AtAEkA8pm5r33F1BwHBACQeTmEEMKTlaH//Zk5X28FEdUG8D6Ay5l5LxE1tKV0usdy/WRIRBdCCC27mlxuBjCVmfcCADPn2rRfH0o8lxq6EEJ4MRvQGcBsIlpFRCN11rcBUIeIFijb3K63EyIaSUTZRJSdl5cXVIHdNXQJ6EII4cFsk0tvZj6gNKXMIaItzLzIaz/dAVwKoCqAZUS0nJm3aXfCzOMBjAeAzMzMIEOy0oYuTS5CCOHBVA2dmQ8oP3MBTAOQ5bXJfgCzmPmU0s6+CEAXOwuqkhq6EELoCxjQiag6EaWprwEMArDRa7PvAfQmokQiqgagB4DNdhcWqGhDF0II4clMk0sjANPIVTVOBDCJmWcS0f0AwMwfMvNmIpoJYD2AswAmMrN30LeFUg6poQshhJeAAZ2Zd0Gn+YSZP/R6/zqA1+0rmj53LxdpQxdCCA+OGykqbehCCKHPuQE9usUQQoiY47yA7h76LyFdCCG0HBfQITV0IYTQ5biALkP/hRBCn/MCOlX0cxFCCFHBeQFd+Sk1dCGE8OS8gC5t6EIIoct5AV0muBBCCF3OC+jugUUS0YUQQst5AV35KeFcCCE8OS6gQ4b+CyGELscFdJIJLoQQQpfzArq0uQghhC7nBXTlp8RzIYTw5LyALhNcCCGELgcGdNdPaUMXQghPjgvopeVnAQB//W4jcvJPRbk0QggROxwX0M+UlgMA9hQUod8/F0S3MEIIEUMcF9CFEELok4AuhBBxwnEBndwdF4UQQmiZCuhElENEG4hoLRFl+9nuQiIqI6Lr7SuiEEIIMxItbNufmfONVhJRAoB/AJgdcqmEEEJYZmeTy8MAvgWQa+M+hRBCmGQ2oDOA2US0iohGeq8koqYArgHwgb+dENFIIsomouy8vDzrpRVCCGHIbEDvzcwZAAYDGEVEfbzWvwXgKWY+628nzDyemTOZObNBgwZBFBeQZ6JCCKHPVBs6Mx9QfuYS0TQAWQAWaTbJBDBZybNSH8AQIipj5u9sLq/EcyGEMBAwoBNRdQBVmPmk8noQgBe12zBzS832nwGYHo5gruw/HLsVQgjHM1NDbwRgmhJIEwFMYuaZRHQ/ADDzh2EsnxBCCJMCBnRm3gWgi85y3UDOzHeGXiwhhBBWOW6kqBBCCH0S0IUQIk44LqDLI1EhhNDnuIDubdWeI/gme1+0iyGEEFFnJZdLTLrug2UAgBsym0e5JEIIEV2Oq6FLN3QhhNDnuIAuhBBCn+MCukxwIYQQ+hwX0Bkc7SIIIURMclxAT01K0F2e9fIveOWnzREujRBCxA7HBfTMc+voLs89WYyPFu2KcGmEECJ2OC6gB8q2OGnFXuTkn4pQaYQQInY4LqAH8vS0Dbj2g6XRLoYQQkRc3AV0ADhWVBLtIgghRMTFZUAXQojKSAK6EELEibgM6NJTXQhRGcVnQGfg65V70f7ZmSguK492cYQQIiLiMqADwFPfbsDp0nIcPVUa7aIIIURExG1AF0KIysaRAX1Elvnc55L7RQhRWZgK6ESUQ0QbiGgtEWXrrL+FiNYr2ywloi72F7VC87rVwrl7IYRwJCszFvVn5nyDdbsB9GXmo0Q0GMB4AD1CLp0NJN2uEKKysGUKOmbWjrVfDqCZHfs1YiVIS5OLEKKyMNuGzgBmE9EqIhoZYNu7Afyst4KIRhJRNhFl5+XlWSmnEEKIAMzW0Hsz8wEiaghgDhFtYeZF3hsRUX+4AnpvvZ0w83i4mmOQmZkZdNVZ5hUVQghfpmrozHxA+ZkLYBqALO9tiKgzgIkArmLmAjsL6a1malI4dy+EEI4UMKATUXUiSlNfAxgEYKPXNi0ATAVwGzNvC0dBtW660Hy3RSGEqCzMNLk0AjBNmVgiEcAkZp5JRPcDADN/CGAcgHoA3le2K2PmzPAUGUioIm0uQgjhLWBAZ+ZdAHz6lSuBXH19D4B77C2aEEIIKxw5UlQIIYQvCehCCBEn4j6gs4wrEkJUEnEf0IUQorKI+4Aug5CEEJWFYwP6kE6NTW0nTS5CiMrCsQF9WJdzol0EIYSIKY4N6GZr3jkFp7BwmyQCE0LEP1vS58aymyesAADkvDo0yiURQojwcm4NPdoFEEKIGOPcgG4xoqePmYGXZ2wKT2GEECIGODagnw2i+8qExbt9lqWPmYHXZ22xo0hCCBFVjg3ooWBm5BcWu9+/N39nFEsjhBD2cGxAD6UN/Zvs/cj82y/YeOC4x/KikjKUlZ8NrWBCCBElzg3oQY4YOnKqBJN+2wsA2J570mNdh3Gz8OCXq0MumxBCREPcd1v01ve1+ThZXGa4fvamwxEsjRBC2MfBNXTXz0SLsxf5C+ZCCOFkjg3oxWXlAIBrujWNyvGPF5WiUC4OQogY4tiAXlTiCujVkhOC3sdjX6/zu/62j1dgstLe7q3Li7PR8+9zgz62EELYzbEBfViXc9CpaS3c26eVLfvbd6TIZ9ni7fkYM3WD4Wekhi6EiCWOfShar0YKfny4t237W7arwLZ9CSFENDi2hm43mQdDCOF0pgI6EeUQ0QYiWktE2TrriYjeIaIdRLSeiDLsL2p4kUxtJIRwOCtNLv2ZOd9g3WAArZV/PQB8oPx0DAnnQgins6vJ5SoAn7PLcgC1iaiJTfuOCKmgCyGczmxAZwCziWgVEY3UWd8UwD7N+/3KMg9ENJKIsokoOy8vtmYRCjagMzNmrD9oKgfMrrxC5J0sDridEEIEw2xA783MGXA1rYwioj7BHIyZxzNzJjNnNmjQIJhd6OrfNvR9kabRJX3MDHy00FwGxlm/H8KoSatNZWwc8K+F6PWKs/quvzlnG+ZtkXQIQjiBqTZ0Zj6g/MwlomkAsgAs0mxyAEBzzftmyrKI+ODW7jhaVIL9R09jd/4pPPm/9Zb34V1Dn7B4l6nPFZwqAQAcOnHG1PZlZ50119Lbc7cDkCn8hHCCgDV0IqpORGnqawCDAGz02uwHALcrvV16AjjOzAdtL62B1KQENKlVFRem10VWel1b9plfWOJ+PX39H4bbBZn0UQghbGemyaURgF+JaB2A3wDMYOaZRHQ/Ed2vbPMTgF0AdgCYAODBsJTWhGDbwv11W3xo0pqwHVcIIewSsMmFmXcB6KKz/EPNawYwyt6iBYeC7IAY6FP7jhShRkoi6lRP9lguFXQhRKyIu5GiwdaUH/7Kfy38ktfmo+/r842PG9xhhRDCNnEX0MPpxBlJxiWEiF0S0IUQIk7EXUAP98PJYOcyFUJEz5nS8kqR7jruAnq4rcw5Gu0iBFR+lvHG7K04XlQa7aIIEROGvLMYHZ+bFe1ihF3cBfRwZ00sO3sWZ7WDg2Kwxv7L5sN4Z94OvPDj79EuiuMVlZShoFDSNTjdrrxT0S5CRMRfQI/AMd6Zt933uEEeeP/RIkxYZG5Uqlll5a6LzNQ1B6SJKERXvPMruv/tl2gXQwhT4i6gR8JPGyoGwYYaLu/6dCVe/mkzDh4/HeKeKrCmVNl7Yr+JKJbtyq8cNTsRH+IuoKs15fo1UrDuuUFhOYZeOhYCYemOfBw/ba3dWn1Qo+5zT8EpbPrjRKhFdDutTKYd67YeOokXf9wkdxRChCDuAroqoQpQq2pSxI534kwpbp64Ag98sQoAsDOvEC9NDxygvFtq+r6+AEPeWRymUsauWz9egU+W7EaupBcWImiOnSTaSMO0VAxo1xD39z0vbMfYkVvofq3G65IyVz70bYdPAgBunrAch08Uo0vz2riycxOPh7WfLdmNo0WleGxgG81+pGYKOHPE7YFjp1GrahJqpMTdfyfhMHFXQ0+oQvjkzguR1dKerIveiorNNWEcPuGqaT7y1RpMyd7nse75Hze509IG2yuHmfHb7iNxfSE4/+mf8EiAlAxaZ0rL8f3ayD8IvvjVebjhw2URPaYQeuIuoHtb99wg/P2aTrbtzzv+Tl0TOO37U99uwIFj1h567swrxJlS34vHiTOlWLazAD+uP4gbP1qGb1btt1ReJyk7y/hh3R/o89p8FJcFvpC++vMWPDp5LZbtLIhA6TxtPmjfcw9hbG9BkftuWPiK+4Beq2oSkhPtO03vALlu3zGf5XrB57Gv11o6zqX/WqibMGzUl6sxYsJyrFeOu6cgur0wzpSW4+lpG7Aj9yQmLNoVcu1Y79N7jxTh0PHAE4io20Qz587KnCNIHzMDf1i8gIvATp4pRZ/X52PMVOsT2KhOFZd5jiOJM3Ef0O2m9vH2pu1NcvOEFT7rjb5Eas39+OlSvDPXs3/70h357s+mj5mB12ZuwZZDrjb6EhNzmEbC5N/2YtKKvbjsjUV4+afNmJK9D2dKyzFjfcTmN3HTdtf86re9uPuzlREvw6QVewEAy3f53iVsO3xSRu+GQP0/tnh7ftD7uOC5WXj5p812FSnmVIqA3rphDdv2NfK/q3SXz9+qTnpNWKXT9ztQf/AXftiEN+Zs011XrtR6x2sGIPmrCJ8prQj2weaHN8v7OvXUtxvw8ozNGDVpNVZoglp+YbFus9OavUfddzn+WKn4EwFjp27A3C25pj/DzBjz7XpTZQGAxdvzsCuv0Ge5vzuUQW8uwjXvLzFdJhEe00w0kzpVpQjoXZrX9njfIC0ljEczH3nmbKqYfNls4iAz4fn/vllnugyh0jtbtbnhpKbpI/Nvv+DiV+f5bHvN+0tx1XtLAp6Xmd9qKK09x0+XYvLKfbjt44q7q2+8HmZr3fbxbxjwr4Uey7R3aWoT3M68QqSPmeGusctApeg7cqok8EYOVSkCureVz1wWtn2fOG2+/fbez7NNbacNVE5q/bO7rKeKy5A+ZgY+X5YT1OeLSspQXFaOU8VlOHHGs+lD/R1rex09EcRk497nvFR5QPvjOuN5aYWwi3SctVk42rY5hNAY7l4uersPxzEfn7IWp5S7mImLd+P2XumWyrT54AkMfnsx2jZKQ07BKRSXnUXOq0N9P2ux7N5NLO4LQ5BNXYeOn8Gi7Xm4MbN5UJ+3W+7JM0hLSULV5IRoF8UxZm48iJ83HsLbw7tF/NiVsoYei/yF7IraY0WwMtONLxLsroWr5/rzBs+Hqmv2HsO2w6426yoGsVIti95zjsFvu0bfbj18EsU63d4CncfCbXlIHzMD+YXF+Oq3ve7lHok3NXvxvjCUm+xZcevHK/Dk/9bHzMPTrJfnYvj44PrYHysqwZIdwT/AjLZlOwvw8a+7LX/u/i9W4/u10bkjk4DuAHptw1Oy/fc/D7elO/Kx8cBxv9sE24VxR24hZmww7iUTjhTJalmN9vyJ8h973pZcjJ26wb38rOYctaf79cp96KeZg3bySuP2eK18JVVveZC/uz0Fp7B0p71BdN1+/39nI3d+uhK3TFzhmHxCqsLiMpwpLceICcvx0vRN0S6OJaYDOhElENEaIpqus64FEc1X1q8noiH2FrNyOqX8R2g/biYAoLScLec6uWXiCt3a4Zq9R33adVftOWKY9fHlGZ5f7JsnrsAV//7VIAD6D7j7jhQhx+vhoHpeczYfxmVvLMT0ILo96sXAOZsO48sVewJ+9kXlP656sVjqVbNU0z086dWurv3dMipq+kt3FiCnoMhkySuYuVTd85+Vug+YAVcuoJsnrMDuGHj4ul1Jg2H14rRh/3Es3p4XeMMw6fjcLMfmU7JSQ38UgFEHzr8CmMLM3QAMB/B+qAWrbIxGGi7aFvoX+1iR71P9a95f6jNw6boPlqHv6wt09zFhsf6tp7b5wYx5Ww7jktfmo98/9Y/z7HcbA+6DAOTkn3LXZv259/NsPDMt8D7VW2Q1oN7n1WxjNNLX44E1s21pB4z2U1J2Fr9szvUpz7wth7F+f0WXy/4Gv99osPo7ufLdX3Hbx7/57seuApng1AkxTAV0ImoGYCiAiQabMICayutaAOSRvk1u/8T3i+1t0x8ncPiE8UjKKhaaKErKzuLq95ag1ytzfdb9vOEgFmz17Nu9Pde3L7Y/f/7MXM8evwjo988FumW0yru3i1Xa2qdewLHSOFRafhZHlbbzzQdPou1ff/YZIdv22Z91P/vnz7Ix7F3fPu6fLtmN9DEz4np0pKhgtob+FoAnARh14XgewK1EtB/ATwAe1tuIiEYSUTYRZeflRe+WKp4QCEPeWew3uHnH81m/H/K7z7X7juGgzlD7B75cjTs/DTz6sqjE1Rtl7mbzA3usUE+n1GvU7rwth3039mPUpNXo/PxsrNlbMejLavO8URu6WWpXzEkr9nr8vj5floPisrOYtyUXz3630T3qVXuMMhM9ql75eQsAYPamQ7q5gUR8CRjQiegKALnMrD9E0mUEgM+YuRmAIQD+S0Q++2bm8cycycyZDRo0CLrQwRjSqXFEjxcpanD2VwHzXqdN/5t3shjLdhYgfcwM08cMVNtT22+/9jMwJxRGD0WtVkLV9AQbPB7uWovonqmUfTuYBkrKpt5ZTVzsmQdHPUUG47/L9+iOeg2UmA2oSOt8/xerDR/w5Z0sxt+mb/LbE8ffHaAR9e8Uzdw6dsh00BSEZmroFwMYRkQ5ACYDGEBEX3htczeAKQDAzMsApAKob2M5Q/aOV5/QXq3qRakk9jLT5DF5ZUU796o9R/GrJhdGYXGZx3ot7weXqtKz/muG2lrk/C25mL/V3pq63X1cxn1fMZm21Rr6Z0ty3K/1augfLNip+7njp0vR6flZWLH7iHJgz/Wzfj9suE9VkcXeI3sMHtI+M20DJv662+/zmh5/n4visvKgnhEYPcANZOG2PLzw4+8hN4uFysyzGtX3a6ObViDgwCJmHgtgLAAQUT8A/8fMt3ptthfApQA+I6L2cAX0mGpTSUzwvHZ9NbInAFctbdSk1dEoUsRoR69e98FSj3XlZxmFBjUooweXgWjrqXf5SZCV8dKcoPYfzsFSBGsP8X7Q9BQ6y2z6yd3GA8dx8kwZ3v7Fd8JxLX+7C+XXMH39HygqKUdKYhXMVlJQnA1w3m3/OhMvDLsAd1yUbvl4hcVllicAuUN5fkQgjLuyg+VjRsOjk61lVbVb0P3QiehFIhqmvP0LgHuJaB2ArwDcyTE488IbN3bBU5e381g2tHOTKJXGfsVl5bpNJ/4CYGFxmaUkVnYKNqdGuBOOdXp+Nk6azK2jZeUL7/2/Y1feKUzS6TFkptdPqUFb+lA/Xe8emrQGT/5vveUANH29uf4Or8/a4pGfqONzs/DRQt+7lfQxM/BQgAqV0fmFYkduYVzmVbcU0Jl5ATNfobwex8w/KK83MfPFzNyFmbsy8+xwFDZU12Y0wwP9wjc1XbSpt+neCMDczYd1R5de/Z792f/CfSm3OhG3FUTmE6V5C1TD1XNI0zZtNS2seqHuZ9DV9HcbJxtXmT3F9+b7Bm/1Aa23QGMOrPRjH/PtevedHzO7H9B7u+yNhXjhx9911zmZjBQF8OJVF0S7CLaYu1k/oP+2+wju/k822v51ZoRLFB7aIPjJr7ttrWmFdLHg0PLuBOJ9QVbvU6zOhmW3tfuO4fEpa/HaTP2AHapygzkI9Exeuc995/fVb/vQYdwsw0lg3M8vNALdLcQ6CegAbu+VjkEdGkW7GCEzyh8RKBe7VYGG/EfSi9M34S82pgvW5pK36izD9DD3cd8Hbkrx5n1BDjYFwujJ+vO0mqkIZ+85ihd/9Owtc/V7SzB19QG8b/AAOFTqnY/6s7i0HKdLyrE7/xRKy88a9rpSe4D5S1m8dEe+R96cYEYoxxIJ6Io3b+qKrPTwTCwdb0aM952RKZp+XPeHpW6X4cJgzUQn/kUzL/p3JhNHTVuj3y3ykyXGCas+X5aDHbkngymWIbXJRU1VceJMGR78chX6/3MBWj/zM56etsHnM/uPanr0GFyoisvKcfPEFbjzs8CD95xCArqiekoiup1bO/CGImCK4Jh7Gh4h+4+aa/rQC0DBsNpF7tcd+R69cgJZu9d49qZTBs8Zxn3/uzuzpZ6Dx09j3b5jOHKqxPRELGoNvLC44u5H+7xBL/HZED9lUKmjcLceCnwB+m7NgbA1KdlJArrGsC7nRLsIcSHPYgKxePH4FHM9RtR5R0O1eu8xy+3nj+hMPK61+eAJd+6fz5cbJzUb7WfSc+8RvFoXvToPV723BK/P2or/mRgYBQDq7rQNTIFamzwGMxls66+c3kZ/vTZsTUp2koCucb6Nc4+Kymffkcg/nBz4xsLAG5nEcOWNv+q9Jfjj2Gm/bepqMjntNIqmjuFnnwWFxYa9UmJF+pgZyLUwanb74ZM+mTvDSQK6CS3rV492EYTQZXW0qBl7CorwmJ8aOFBRQzY7jaIZ3f/2i9/mGu21wHsswkKdUa5q+t5AikrKLWUNXZljvpPBwDcX4WYlhfXPGw7alo3TiAR0DaMBKzE4RkqIsAo0w1Kog7uM/k8ZpScY8+16vDPXeFTtDp0UGH8obeRHCgMPYNNOWhKItrln4uJdSB8zQ7frrHY8wye/7sYDX65Gy7E/4V+zt5o+llUS0DXCPf+mEE4R7v8LZmdwCnb7YGwwOTOT9uKhXmROl5R79qwB0Pe1ihmr/tBMHPPveTtCKaZfEtA1jL7DzetWi2g5hIi2QDXwvUeKLLUlh5u/u2izF6frP1waeCMAb8zZpru89z/me7wvCDK1RSgkoJvw7ogMfHhr92gXQ4iI2XIocNqArL+HPsGIyl8XTO+pEvX8bYbRZGqxIVKtthLQNbQj76bc1wv393XlfalVLQmXd4zPfOpCqLS13EjnMLecpdBCk9DjU+wbSayVX1gcc7nereWzjHPa70hWy7rIalkXYwZXZGdMS03EyRj7Awphl6M6c89WJsUWcwI9933sJfeSGroFKYny6xLx66lv7RnBGgmxkPpWO2L69z9iI7+RRCiNQA9PJo/sibaN0iJTGCGEJct2FkTsWD9vOOhxR293ArxgSUDXUNvQa1dL0l1/fsM0zHqsTySLJIQwaUqY5rDV88CXnml2A+XCPxah5ixpQ/fy2nWd0aOVZF0UwmmmrYnsfJ5VNLf0gSYoN5vhMlQS0L3ceGHzoD5Xq2pSWGfSEULEFo9aeYyMJpcmF5tUTUoAALx1U9eA297QvVm4iyOECLPZmsRkZ2LgIS0gAT0o3c+tg2eGtHe/z2hRG38Z1AYAcNF59QAAfds0MPy8pBgQIr6MX7Qr2kUAIE0uQfn2gYsAANd1b4aCwmI0rVMV1ZITcUOmq7km59WhAGA4i064Z60XQsS2opIyVEu2P/xKDT0Edasno3WjtLD8YURsq56cEO0iCAd7bWZ4Mi6aDuhElEBEa4housH6G4loExH9TkST7Cti/GnfRPqyO9mU+3rh2gx5DiKC99nSnLDs10oN/VEAuhlwiKg1gLEALmbmCwCMtqFsjvfPG7rg3yO6oVereh7L77goPaj9DerQyIZSiVClJlUJ+m8oRDiZCuhE1AzAUAATDTa5F8B7zHwUAJg5157iOdv13ZvhSp15Sknnqeg13ZoG3F/bxvFbs+/YtGa0ixC0+jWSo10EIQCYr6G/BeBJAEZ9c9oAaENES4hoORFdrrcREY0komwiys7L850yqjIbd0WHgNskJcTvI4/Rl7aJdhFMcz3Uruh3fFvP9KiVRQitgBGCiK4AkMvMq/xslgigNYB+AEYAmEBEtb03YubxzJzJzJkNGhh364s3Zropmtnmnktahl6YGFXPQbXclg0855hNlqRtIkaY+SZeDGAYEeUAmAxgABF94bXNfgA/MHMpM+8GsA2uAC9s5K83zRd39/B4X6uqfj6aWFUtOdHd3TOS/nFdJ0vbN6qZghopnn8HGVcgYkXAgM7MY5m5GTOnAxgOYB4z3+q12Xdw1c5BRPXhaoKJjZ72MeDqAO3j7ZvURHUlSHg/QDWrd+v6+H7UxbikdX0AwLMmmnBixdBOTdCmUQ1b9tWqfvXAG2ncdGELbHlJt4VQ1yOXuuopMTLSWwgPQd8rEtGLRDRMeTsLQAERbQIwH8ATzBy5XJYx7sbM5tj9yhDD9WMHt0NSQhXkvDoUI3q0sLz/q7u6Hrx2aV4bDWqk+KxvmOa7LNIeGXC+4br7+56n+6DYqkta18d1BmkVujb3aQF0S01KMNVskvPqUNzS41yf5cFehIWwm6WAzswLmPkK5fU4Zv5Bec3M/Dgzd2DmTsw8ORyFdTKjgFW3erK7Vh2sFvV8a6Xao3U4x7cHyX19WkW07ffB/sYBvYoNxRg7uB3+e3cPNK6Zqrt+SKfwTSHYxc/FIhpayKTmlZY8zYmy1c8O9Aj26ryOjWp61qrv69vK57MP9jtP/VDF580emICOOoE+FPf0Nn5om5qU4BFsX7u+s6Yo/mvnt/YMfNdSrvwOhnZugleu7YRqfkZyNq1d1XehxSYUdfPkGOp5dGOm6+7kwX7nYdZoydtfGcXOt1F48A5yKYmeAeqb+3v5LAMqkoK1b1ITaSmJqFfds/fIo0obMIHw6Z1Z7mRiWkYPJze9+Ce/Za4Z4EFsev2KmqO2GahBgCah+/qc53c9AJxVElKnJiVgRFYL1Ez1LCe6ABEAABIqSURBVEu42rzPrec6p1H9Pcv46V0X+mzr/TA1nOJ5zEI8CNeDdAnoMaqFEijUgFzF6wtwYXpd95dCG6uu7tYU658fhA7n1MSacQOx4ulLPT6nNrMQAbWqJbkvAK0aBH6YaCVnzRN/auuz7P1burtfa8usF9Cvzah4kGz05Z9we6b7tfcEA1+N7OnxXruabYju3rt44k/tPC6E3c+t4/OZy9o3DPm4Zkivm9gXrj+RBPQoGdqpCd67OcNwfaOaqdj+8mCMyHI1N1TR+V9q9KVQa6eJCVWQaNAkoH52eFYL9GvbAJO9AqC3+joPW70laK462vTBavNK3erm+5rf1tP34aO3rPS6mPrgRbjovHq46+J0j3Ut/fR2+ezPWT7LBgaZVsFK8NT7G4rKKdAMR8GSgB4l792SgaGdmxiuJ7hGhqqzonjX0K3Q1iav7HwOqiYl4HqlN0itqkn47K4sNEzzfJh4ZZdzcIWmfF/c4xkE/z2im89xtM0o2oFCN2ZanwWKiNwZDdNS9ZtyGIyMFnUw6d6ehtu4t1V+B0kJhDY6E32/cVMXS+Vjq43uiPwApMVP9sd1ISYRi9RdhbCHBPQY430rP6STK6gO7GDcSyNQC4JaM+5+bh20qFcNm1+6HK0a+O/3/e8R3fCu5g6iXWPPB6h6OWq0mtSqioVP9MOOlwcbrNfvjaJHr2KbklglYBDXUm9UjAJcSmIC1jw70PT+1N+5ldz2d10c+khf77Z6rSa1XA9761Rz/b2b162Ge/uEdsxEO7ogiYiRRN4xRq09ql0ZOzatZfiQsqIN3X9ET1FqhtcHmPru2oymmLo6+Il2vbtmnqvTnVKl29MEQJdmtbBu/3EAFe3e3k0Vzwxpj3v7+Pb68acKETY8P8jvc4A6fpqEOjTR7xFkphXl/Vsy0LJ+dfffwYp/3dAFvc6rh4tenQcAqJGifxFrWrsqHhpwPto2TvNoPmqUZv7CqSLSXLCklchRJKDHmA7n1MTacQNRu1rg9mazg3HMPgN8/fouePlq/0PhP7w1AyXlFTvs0KQmikrKkFNQZCpgVU9OwKmScsOy6xXVe8t2QeSTZzZuutEz+7E+GPTmIgBA52a18MNDvX32F8iIrObILyxx32Xtyis0X2C4+rcbDZTSSktJxOzH+iApoYr7WKo61ZNRv0YK8guLLR37/Vsy0LZxGu76dKWlz4nokoAeg8wE83BIqEKo6tV/e/WzAz0edl7esSJgzHmsDxrXSkUVIvxnWQ6GdGqCh79a4/cY9/Zphbd+2Y6UxCoYO7gdOjczNyinZ6u6WL7rCIZf2Bw9bRiZ+dp1nf12s9RrZ9eqWdX1X+eCc2rprk9OqIJXru3sscz7GjBz9CW4/K3FeO/mDIyatNpj3eCOjTH6Mv8ZKLs2r421+46hTvVkd+oIPdl/vQzXfbAUp4rLsOXQSb/7BFzNfOqFYe+RooDbq9aOG4iuL84xvb2wnwT0CPr8z1k4fOJMSPsY0K6hz8jSQLXFUG6b/fVMaa0Jeg/2Mx4JqjX6sjbuQHVfX//9y7W3/ZNH9jK1f62kBEKpcjfh3Sx144XWH9RqNatTDVMfvMinKWb6w71RIyURqUm+YwS8/07tGtdEzqtDcayoxGfbD27t7rPMmzp4qp2JPufqPLhG89xqDQ/idzP94d6oXS0Zj17aGm/P3W7588IeEtAjqE+b0FMGf3JnxYCVq7qeg49/3R2wF0laquvPXFUnyNht+dhLA29kgivjOCuvg7siuZp1lIAehm5iGS18+5p3bKpfY3fRL0Sw59cgLQVfj+wZ4JjWBdO9Ui3DYwPbxFRAf6j/+Xh3/o5oFyNiJKA7WLM61bDaRM+Mxwe2RcO01IA9U+zQ2ELvFT1DOjXB+v3HLfWCMaINS6HkN/HXp90Kw4uKpb7snu97hCExmFFxMlrURlbLevhw4U5L+0uvVw05BeabbuzknUIj3kmfpEqganIC7u3TyqMtPFbd16cVfn/hT2hYM9U9mCnUnhZf3tMDgzsZ9/nXmjyyJ6Y+eJHHsmZ19HvkWBVoMElSgv8TPadWKm7vlY40G1MI6OXN1z6wHtCuoh/61AcvxpjB7SwfI5ibo5qp9pxjFQd85+0kAV3EFCJyP+CbPLInXru+s257tBlq00G3FuazIfZsVc/dlHKVkpY42CYRb2oTUnJiFcx4pKLXjBo//R1n2dgBmPVYH1RNTsBLV3e0pTwAdHP5aC+g3g/JrbjbT7K2QPQuAs9daT3Hf0Il63cpAV3ErGZ1qgU1ylR1XkNXU0mwAfm8AIOvrFKbXFrVr+7RO0ZbutmP9cEvj/tmSmxSq6q726Vaq25kkCrYn0EdGuHyCzwHqf33bs9RwFba0J8e0g5PD9Gvtd9qIn2DFVd19Zwoxkwzmr8auppWI55IQBdx6z93ZeHTOy8MupZp9+CapkrTzUivQVHaJo42jdJwfkP/vVb6tW2At4d3xV8GWZ9Ye/ztmXj5mooaPhFwSWvPh/Vmz/ffI7phZJ/zMNJENkyVdkCZ34Bsop3m79d0Cjhtofbi1KhmisfF8u/XdPS5mKnaGwwki3US0EXcqlcjBf3bBZ+LpKKXjT1qpiYh59WhuNYr/QD5vPCPiHBV16a66ZPNqFcjBW8quWsuTK8LAHhdk5/ebLOz0UP2do3TPDJoane3ZMwA9+vbe/nW4P8y0HWRYgDTHrzIcPKQF6+6AL1NTAyjPZcmtap6XCyJyHAmK6c2vUtAF8KA+hAz3A/WotHMe023Zvj1qf6486J0AMANmc01wa2iQIOCyEI5c3Qf/OaVtvmy9o0wVHkwrWbGJCJM1KRAvimzOe5U1jEzurWog9YNK5q9tGmPu5gckKbNNhrNaWB3vzLE584sHKTbohAG1ABi10NRI+Hev5FmdTybPCpy51Qsu6prU1zesbHluwHv1A4T78jU3e4yzQXjH9d3Rln5WQDAQwNamz5W45qpOKQZsNekVioOHne9T9P0lrEjD74Z3c+tg1V7jnosIyI8PaQ9xi/aFdZjSw1dCAN2pC62Itp3+e4LmFcw9g7mvzzeB28P7xrCcYzXJSqTpT+gTK+o/vSndjXPrpfaUbbaM9E7rh2TkwOuqQ9/euQS/N+gNu5RudEgAV0IA5FqclFFu4fdv27ogis6N8EFAeaaPb9hmk+PEz1Na1dF64Y1DLtZmjnd8xrUCDgxyoTbMzH6sooafeOaqVj8ZH9c3fUcXHReRTu7mQFiX4/siV8e72uiZEBWy7oVb9iVWE+9s9CbAzgSTAd0IkogojVENN3PNtcREROR/v2VEA6i5khpGyBRV7xo3SgN796cgSSbJr5OTqyCOY/39elFY5VaAzfqTtm8bjWMvqwN6iuTqiRUITSvWw1vDe/mManIq9f5ZhL13mOPVvVwfsMaHhfX70Zd7H7dp00D/HVoe7x5UxdMuc84v9CwCIzK1mOlDf1RAJsB6F6+iShN2WaFDeUSIuqu6toUHZrU9EhCFg7BzH5Umfz37h6YvyXXI1+9Ojm3VplyS+U9IvrTuy5E64Y1LM2J++6IDPT75wIA8OgJUy0pAfdcEnzt+76+rfDRwl3o0sze/DsqU5diImoGYCiAiX42ewnAPwCElk5QiBgS7mAOVNQ8ndr32aqKtnpz2zetXdU9SClL6Wapl2JanczDO4VC/7YNfR4AB5Lu1TzTS8mZYzZXkdGD7kvbNVLKGJ7WbrOXrLcAPAlA99tNRBkAmjPzDCJ6wmgnRDQSwEgAaNEi/kZpCRGM1KQEfHVvT7QPYuIOJ1KfTQSTW+jr+3oaPlT9/O4sLN9VYGkik0DUi0Pftg2wbFeB4bywVu+ywvW8JGBAJ6IrAOQy8yoi6qezvgqANwDcGWhfzDwewHgAyMzMlPtMIRS9dHKqxKuz7Dlga8mYAbo54fUQkWEwbFQz1dTDWi3tRaW614jiT+7MRGtlIFLFHLL6xg5ub+p4dZTnAeG68zNTQ78YwDAiGgIgFUBNIvqCmW9V1qcB6AhggdIFqDGAH4hoGDNnh6PQQgjnUmtyapfBprWrGs4xG26pSQl448Yu6Naijk/K5gHtKvrIu2vgXhH92wd6oUZKEpp7pTFITtQP/a0bpWHyyJ6WEsZZEbAhh5nHMnMzZk4HMBzAPE0wBzMfZ+b6zJyubLMcgARzIYSuakr2zEhMuGLGtRnN0LJ+db9ZPStq6J6Buvu5ddFWZ8ao8xum4YVhF+juq2erekGnbQgk6JZ5InqRiIbZWRghRPx7fFAbPPGntri6m7XmkWi6IbMZ2jVO080/Y+QOJa1CJFka+s/MCwAsUF6PM9imX6iFEkLEr2rJiRjV39wctLGiYVoqZo72TWsca2SkqBBCxAkJ6EIIESckoAshRJyQgC6EEGHSqWl4hvgbkXzoQggRJpPu7YHck8URO54EdCFEpfLS1R3DlhzLW1pqkq2pCAKRgC6EqFRu62m+L7nTSBu6EELECQnoQggRJySgCyFEnJCALoQQcUICuhBCxAkJ6EIIESckoAshRJyQgC6EEHGC2GjG1XAfmCgPwJ4gP14fQL6NxYmmeDkXOY/YIucRe+w6l3OZuYHeiqgF9FAQUTYzZ0a7HHaIl3OR84gtch6xJxLnIk0uQggRJySgCyFEnHBqQB8f7QLYKF7ORc4jtsh5xJ6wn4sj29CFEEL4cmoNXQghhBcJ6EIIESccF9CJ6HIi2kpEO4hoTLTL442IPiGiXCLaqFlWl4jmENF25WcdZTkR0TvKuawnogzNZ+5Qtt9ORHdE4TyaE9F8ItpERL8T0aNOPBciSiWi34honXIeLyjLWxLRCqW8XxNRsrI8RXm/Q1mfrtnXWGX5ViL6UyTPQ1OGBCJaQ0TTHX4eOUS0gYjWElG2ssxR3y3l+LWJ6H9EtIWINhNRr6ieBzM75h+ABAA7AbQCkAxgHYAO0S6XVxn7AMgAsFGz7DUAY5TXYwD8Q3k9BMDPAAhATwArlOV1AexSftZRXteJ8Hk0AZChvE4DsA1AB6edi1KeGsrrJAArlPJNATBcWf4hgAeU1w8C+FB5PRzA18rrDsr3LQVAS+V7mBCF79fjACYBmK68d+p55ACo77XMUd8tpQz/AXCP8joZQO1onkdE/4g2/PJ6AZileT8WwNhol0unnOnwDOhbATRRXjcBsFV5/RGAEd7bARgB4CPNco/tonRO3wMY6ORzAVANwGoAPeAasZfo/b0CMAtAL+V1orIdeX/XtNtFsPzNAMwFMADAdKVcjjsP5bg58A3ojvpuAagFYDeUziWxcB5Oa3JpCmCf5v1+ZVmsa8TMB5XXhwA0Ul4bnU9Mnadyu94Nrtqt485FaaZYCyAXwBy4aqXHmLlMp0zu8irrjwOohxg4DwBvAXgSwFnlfT048zwAgAHMJqJVRDRSWea071ZLAHkAPlWawSYSUXVE8TycFtAdj12XYMf0FSWiGgC+BTCamU9o1znlXJi5nJm7wlXDzQLQLspFsoyIrgCQy8yrol0Wm/Rm5gwAgwGMIqI+2pUO+W4lwtW8+gEzdwNwCq4mFrdIn4fTAvoBAM0175spy2LdYSJqAgDKz1xludH5xMR5ElESXMH8S2aeqix25LkAADMfAzAfrqaJ2kSUqFMmd3mV9bUAFCD653ExgGFElANgMlzNLm/DeecBAGDmA8rPXADT4LrQOu27tR/AfmZeobz/H1wBPmrn4bSAvhJAa+XJfjJcD3t+iHKZzPgBgPrk+g642qPV5bcrT797Ajiu3KrNAjCIiOooT8gHKcsihogIwMcANjPzG5pVjjoXImpARLWV11Xheg6wGa7Afr3Beajndz2AeUot6wcAw5XeIy0BtAbwW2TOAmDmsczcjJnT4frez2PmW+Cw8wAAIqpORGnqa7i+ExvhsO8WMx8CsI+I2iqLLgWwKarnEemHITY8iBgCV4+LnQCeiXZ5dMr3FYCDAErhuoLfDVfb5VwA2wH8AqCusi0BeE85lw0AMjX7+TOAHcq/u6JwHr3hulVcD2Ct8m+I084FQGcAa5Tz2AhgnLK8FVyBbAeAbwCkKMtTlfc7lPWtNPt6Rjm/rQAGR/E71g8VvVwcdx5Kmdcp/35X/x877bulHL8rgGzl+/UdXL1UonYeMvRfCCHihNOaXIQQQhiQgC6EEHFCAroQQsQJCehCCBEnJKALIUSckIAuhBBxQgK6EELEif8HkzWNVkLlmrkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the entire model with discriminative learning rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsched = sched_1cycle([lr/2., lr/2., lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_flat</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_flat</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.394669</td>\n",
       "      <td>0.250716</td>\n",
       "      <td>4.317773</td>\n",
       "      <td>0.260431</td>\n",
       "      <td>14:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.326853</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>4.271071</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.269067</td>\n",
       "      <td>0.263070</td>\n",
       "      <td>4.229544</td>\n",
       "      <td>0.269338</td>\n",
       "      <td>14:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.219788</td>\n",
       "      <td>0.267743</td>\n",
       "      <td>4.194589</td>\n",
       "      <td>0.272833</td>\n",
       "      <td>14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.178052</td>\n",
       "      <td>0.271664</td>\n",
       "      <td>4.166258</td>\n",
       "      <td>0.275681</td>\n",
       "      <td>14:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.144542</td>\n",
       "      <td>0.274591</td>\n",
       "      <td>4.143349</td>\n",
       "      <td>0.277895</td>\n",
       "      <td>14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.114541</td>\n",
       "      <td>0.277206</td>\n",
       "      <td>4.124201</td>\n",
       "      <td>0.279645</td>\n",
       "      <td>14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.088627</td>\n",
       "      <td>0.279393</td>\n",
       "      <td>4.109105</td>\n",
       "      <td>0.281351</td>\n",
       "      <td>14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.070786</td>\n",
       "      <td>0.280875</td>\n",
       "      <td>4.101879</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>14:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.061327</td>\n",
       "      <td>0.281740</td>\n",
       "      <td>4.100467</td>\n",
       "      <td>0.282265</td>\n",
       "      <td>14:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the encoder and the vocab for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model[0].state_dict(), path/'finetuned_enc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab, open(path/'vocab_lm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model.state_dict(), path/'finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60002"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num, proc_cat = TokenizeProcessor(), NumericalizeProcessor(vocab=vocab), CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore padding of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3352]), torch.Size([64]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility functions need the lengths of the sequences as they are applied after the embedding (and the padding can't be seen anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = x.size(1) - (x == 1).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3352, 1741, 1558, 1528, 1444, 1348, 1335, 1326, 1316, 1313])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = nn.Embedding(len(vocab), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3352, 300])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor is padded, however!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `PackedSequence` object that contains all of our *unpadded* sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(test_emb(x), lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29935, 300])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explaining the \"29935\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29935)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padding is simply left out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explaining the `packed.batch_sizes`:**\n",
    "\n",
    "The longest sequence is 479 long, we have that many batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(packed.batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64, 64, 64, 64])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed.batch_sizes[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first few steps, all sequences are long enough and \"still left\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13, 10,  6,  2])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed.batch_sizes[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([479, 479, 478, 478, 478, 478, 477, 477, 477, 477, 476, 476, 476, 475,\n",
       "        474])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 sequences with 479 tokens. These remain until the last step. There are 6 sequences that have >= 478 tokens. There are 10 sequences with >= 477 tokens. There are 13 sequences with >= 476 tokens.\n",
    "\n",
    "The `packed.batch_sizes` tensor says \"how many sequences are left at a certain length\". In the beginning all 64 sequences are long enough. But later only a few sequences are left, on the last step only 2 are left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `packed` object can be passed to any RNN directly while preserving the speed of CuDNN without wasting compute on padding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lstm = nn.LSTM(300, 500, 4)  # input_sz, hidden_sz, n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, h = test_lstm(packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(h)  # hidden and cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 500])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[1].shape  # num_layers, bs, and hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pad the packed sequence:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked = pad_packed_sequence(y, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 479, 500])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked[0].shape  # the padded tensor of hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(unpacked[1] == lengths).all()  # contains the lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's adapt the AWD-LSTM accordingly:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AWD_LSTM1(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs, self.emb_sz, self.n_hid, self.n_layers, self.pad_token = 1, emb_sz, n_hid, n_layers, pad_token  # bs = 1 to make it reset h\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(input_size=emb_sz if l == 0 else n_hid, \n",
    "                             hidden_size=(n_hid if l != n_layers - 1 else emb_sz),\n",
    "                             num_layers=1, batch_first=True\n",
    "                            ) for l in range(n_layers)]\n",
    "        \"\"\"\n",
    "        (Pdb) self.rnns[0]\n",
    "        LSTM(300, 500, batch_first=True)\n",
    "        (Pdb) self.rnns[1]\n",
    "        LSTM(500, 500, batch_first=True)\n",
    "        (Pdb) self.rnns[2]\n",
    "        LSTM(500, 300, batch_first=True)\n",
    "        (Pdb) self.rnns[3]\n",
    "        \"\"\"\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        # Initialize embedding\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        \n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "        mask = (input == self.pad_token)\n",
    "        lengths = sl - mask.long().sum(1)\n",
    "        n_empty = (lengths == 0).sum()\n",
    "        \n",
    "        if n_empty > 0:\n",
    "            input = input[:-n_empty]\n",
    "            lengths = lengths[:-n_empty]\n",
    "            self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]\n",
    "\n",
    "        # self.hidden ist list of len 3 of tuples of len 2 of tensors of shape [1, 64, 500/300]\n",
    "        # input shape [64, 70]\n",
    "        raw_output = self.input_dp(self.emb_dp(input))  # [64, 70, 300], dropout on input along seq dimension\n",
    "        \n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]\n",
    "            # new_h has shape [1, 64, 500] for l = 0 (final hidden state), raw_output [64, 70, 500] for l == 0\n",
    "\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)  # RNNDropout along seq dim on h's\n",
    "            outputs.append(raw_output)\n",
    "            new_hidden.append(new_h)\n",
    "             \n",
    "        # here new_hidden is list of len n_layers with tuples of two tensors h,c each\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs, mask  # with and without dropout\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randint(low=0, high=30000, size=(64, 70)) # (bs, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm = AWD_LSTM1(30000, 300, 500, 3, vocab.index(PAD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm.bs = bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_outputs, outputs, mask = awd_lstm(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, torch.Size([64, 70, 300]), torch.Size([64, 70]), torch.Size([64, 70, 1]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs), outputs[-1].shape, mask.shape, mask[:,:,None].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat pooling\n",
    "For the classification head of the model we will use the last hidden state, the average of all hidden states, and the maximum of all the hidden states. The trick os to, once again, ignore the padding in the last element/average/maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs, mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        \n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)  # replace padded parts with 0\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        # shape [64, 300]\n",
    "        \n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        \n",
    "        x = torch.cat([\n",
    "            output[torch.arange(0, output.size(0)), lengths - 1],  # last hidden state before padding\n",
    "            max_pool,\n",
    "            avg_pool\n",
    "        ], 1)\n",
    "        \n",
    "        # shape [64, 900]\n",
    "\n",
    "        return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 500, 2\n",
    "tok_pad = vocab.index(PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=tok_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.bs = bs\n",
    "enc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, c = pool(enc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3352, 300]), torch.Size([64, 900]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     7,  1145,  ..., 15375,    24,     3],\n",
       "        [    2,   194,    50,  ...,     1,     1,     1],\n",
       "        [    2,     7,    47,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    2,     7,    28,  ...,     1,     1,     1],\n",
       "        [    2,     7,   361,  ...,     1,     1,     1],\n",
       "        [    2,     7,    17,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All sequences have padding except the first one which is the longest. When unpacking/padding the packed sequence, PyTorch puts 0s everywhere we had padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3352, 300])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sum(dim=2) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near((output.sum(dim=2) == 0).float(), (x ==tok_pad).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last hidden state for each sequence is generally not the last element of `output`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3352])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3352, 300])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 900])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bs):\n",
    "    length = x.size(1) - (x[i] == 1).long().sum()\n",
    "    out_unpad = output[i,:length]\n",
    "    test_near(out_unpad[-1], c[i,:300])\n",
    "    test_near(out_unpad.max(0)[0], c[i,300:600])\n",
    "    test_near(out_unpad.mean(0), c[i,600:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooling layer properly ignores the padding. Let's feed the output into a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p !=0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(nn.Module):\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        mod_layers = []\n",
    "        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n",
    "        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n",
    "            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "\n",
    "        self.layers = nn.Sequential(*mod_layers)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs, mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        \n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)  # replace padded parts with 0\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])          # shape [64, 300]\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        \n",
    "        x = torch.cat([\n",
    "            output[torch.arange(0, output.size(0)), lengths - 1],  # last hidden state before padding\n",
    "            max_pool,\n",
    "            avg_pool\n",
    "        ], 1) # shape [64, 900]\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot pass the entire texts to the AWD_LSTM or we might get OOM error. We have to cut them into chunks of lengths bptt to regularly detach the history of our hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "??SequentialRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(t, bs, val=0.):\n",
    "    if t.size(0) < bs:\n",
    "        return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, module, bptt, pad_idx=1):\n",
    "        super().__init__()\n",
    "        self.bptt, self.module, self.pad_idx = bptt, module, pad_idx\n",
    "        \n",
    "    def concat(self, arrs, bs):\n",
    "        return [torch.cat([pad_tensor(l[si], bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "        self.module.bs = bs\n",
    "        self.module.reset()\n",
    "        raw_outputs, outputs, masks = [], [], []\n",
    "        \n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o, m = self.module(input[:, i: min(i+self.bptt, sl)])\n",
    "            masks.append(pad_tensor(m, bs, 1))\n",
    "            raw_outputs.append(r)\n",
    "            outputs.append(o)\n",
    "\n",
    "        # outputs is a list of lengths #chunks of lists of length n_layers of tensors of shape [bs, bptt, 500/300]\n",
    "        # self.concat returns a list of length n_layers of tensors of shape [bs, length of entire document, 500/300]\n",
    "        return self.concat(raw_outputs, bs), self.concat(outputs, bs), torch.cat(masks, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3352])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SentenceEncoder(enc, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, o, m = s(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r), len(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3352, 500]),\n",
       " torch.Size([64, 3352, 500]),\n",
       " torch.Size([64, 3352]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0].shape, o[0].shape, m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(r[1] == o[1]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, dtype=torch.uint8)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(r[0] == o[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, \n",
    "                        input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):\n",
    "    rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    \n",
    "    enc = SentenceEncoder(rnn_enc, bptt)\n",
    "    if layers is None: layers = [50]\n",
    "    if drops is None: drops = [0.1] * len(layers)\n",
    "    layers = [3 * emb_sz] + layers + [n_out]\n",
    "    drops = [output_p] + drops\n",
    "\n",
    "    return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 300, 2\n",
    "dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, *dps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM1(\n",
       "      (emb): Embedding(60002, 300, padding_idx=1)\n",
       "      (emb_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60002, 300, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(300, 300, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(300, 300, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.10000000149011612)\n",
       "      (2): Linear(in_features=900, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_splitter(m):\n",
    "    enc = m[0].module\n",
    "    groups = [nn.Sequential(enc.emb, enc.emb_dp, enc.input_dp)]\n",
    "    \n",
    "    for i in range(len(enc.rnns)):\n",
    "        groups.append(nn.Sequential(enc.rnns[i], enc.hidden_dps[i]))\n",
    "    \n",
    "    groups.append(m[1])\n",
    "    \n",
    "    return [list(o.parameters()) for o in groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model[0].parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback,accuracy),\n",
    "       CudaCallback, Recorder,\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].module.load_state_dict(torch.load(path/'finetuned_enc.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, F.cross_entropy, opt_func=adam_opt(), cb_funcs=cbs, splitter=class_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsched = sched_1cycle([lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.346226</td>\n",
       "      <td>0.851160</td>\n",
       "      <td>0.270373</td>\n",
       "      <td>0.886440</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model[0].parameters():\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsched = sched_1cycle([lr/8., lr/4., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.282283</td>\n",
       "      <td>0.882800</td>\n",
       "      <td>0.227641</td>\n",
       "      <td>0.908600</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248779</td>\n",
       "      <td>0.900240</td>\n",
       "      <td>0.228977</td>\n",
       "      <td>0.906480</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(2, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.239340</td>\n",
       "      <td>0.902880</td>\n",
       "      <td>0.216996</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.230223</td>\n",
       "      <td>0.908880</td>\n",
       "      <td>0.207723</td>\n",
       "      <td>0.916640</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211512</td>\n",
       "      <td>0.917240</td>\n",
       "      <td>0.198096</td>\n",
       "      <td>0.920720</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.193093</td>\n",
       "      <td>0.924920</td>\n",
       "      <td>0.198282</td>\n",
       "      <td>0.921760</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.181635</td>\n",
       "      <td>0.927680</td>\n",
       "      <td>0.200793</td>\n",
       "      <td>0.920120</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = learn.model.eval()(x.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ind = []\n",
    "for inp in x:\n",
    "    length = x.size(1) - (inp == 1).long().sum()\n",
    "    inp = inp[:length]\n",
    "    pred_ind.append(learn.model.eval()(inp[None].cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert near(pred_batch, torch.cat(pred_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on the padded batch gives the same as predicting on the individual unpadded documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastaiV1",
   "language": "python",
   "name": "fastaiv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
