{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layerwise Sequential Unit Variance (LSUV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()\n",
    "\n",
    "x_train, x_valid = normalize_to(x_train, x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh, bs = 50, 512\n",
    "c = y_train.max().item() + 1\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view = view_tfm(1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [\n",
    "    Recorder,\n",
    "    partial(AvgStatsCallback, accuracy),\n",
    "    CudaCallback,\n",
    "    partial(BatchTransformXCallback, mnist_view)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [8, 16, 32, 64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True)\n",
    "        self.relu = GeneralRelu(sub=sub, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "    \n",
    "    @property\n",
    "    def bias(self): return -self.relu.sub\n",
    "    \n",
    "    @bias.setter\n",
    "    def bias(self, v): \n",
    "        self.relu.sub = -v\n",
    "    \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn, run = get_learn_run(nfs, data, 0.5, ConvLayer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper **All You Need is a Good Init:**\n",
    "\n",
    "* Introduces Layer-wise Sequential Unit-Variance (LSUV).\n",
    "* Initialize neural net with the usual technique, then pass a batch through the model and check the outputs of the linear and convolutional layers. \n",
    "* Rescale the weights according to the actual variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalized.\n",
    "* Repeat this process until satisfied with the mean/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's get a baseline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [1.42551203125, tensor(0.5098, device='cuda:0')]\n",
      "valid: [0.2484116455078125, tensor(0.9214, device='cuda:0')]\n",
      "train: [0.2205297265625, tensor(0.9318, device='cuda:0')]\n",
      "valid: [0.117218310546875, tensor(0.9649, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to get one batch of a given dataloader with the callbacks called to preprocess it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn,run = get_learn_run(nfs, data, 0.5, ConvLayer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch(dl, run):\n",
    "    run.xb, run.yb = next(iter(dl))\n",
    "    for cb in run.cbs: cb.set_runner(run)\n",
    "    run('begin_batch')\n",
    "    return run.xb, run.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch(data.train_dl, run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all convolutional and linear layers. Modules in PyTorch can form a tree structure, so we have to recurse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_modules(m, cond):\n",
    "    if cond(m): return [m]\n",
    "    return sum([find_modules(o, cond) for o in m.children()], [])\n",
    "        \n",
    "def is_lin_layer(l):\n",
    "    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)\n",
    "    return isinstance(l, lin_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = find_modules(learn.model, lambda o: isinstance(o, ConvLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConvLayer(\n",
       "   (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " )]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function that grabs the mean and std of the output of a hooked layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stat(hook, mod, inp, outp):\n",
    "    d = outp.data\n",
    "    hook.mean, hook.std = d.mean().item(), d.std().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`def register_forward_hook(self, hook)` with `hook(module, input, output)` and the `Hook` class doing `self.hook = m.register_forward_hook(partial(f, self))` (where `f` might be `append_stat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "??nn.Module.register_forward_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "??Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn.model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the means and the stds of the conv layers of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3720621168613434 0.8526896238327026\n",
      "0.28457432985305786 0.6383782029151917\n",
      "0.3149712085723877 0.5545002818107605\n",
      "0.3181527853012085 0.48824334144592285\n",
      "0.22887423634529114 0.3585847020149231\n"
     ]
    }
   ],
   "source": [
    "with Hooks(mods, append_stat) as hooks:\n",
    "    model(xb)\n",
    "    for hook in hooks:\n",
    "        print(hook.mean, hook.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Means are not 0 and the stds are not one. We first adjust the bias to make the means 0. Then we adjust the weights to make the stds 1 (threshold 1e-3).\n",
    "\n",
    "Here, `model(xb) is not None` does nothing but making sure that the batch is passed through the network repeatedly computing the activations and updating the hooks untill we fall below the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lsuv_module(m, xb):\n",
    "    h = Hook(m, append_stat)\n",
    "\n",
    "    while model(xb) is not None and abs(h.std - 1)  > 1e-3: m.weight.data /= h.std\n",
    "    while model(xb) is not None and abs(h.mean) > 1e-3: m.bias -= h.mean\n",
    "\n",
    "    h.remove()\n",
    "    return h.mean, h.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute that initialization on all the conv layers in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.2503794383510467e-08, 0.9999998807907104)\n",
      "(3.740495557735812e-08, 0.9999999403953552)\n",
      "(6.577465683221817e-09, 1.0000001192092896)\n",
      "(-3.003515303134918e-08, 1.0)\n",
      "(-1.1175870895385742e-08, 0.9999999403953552)\n"
     ]
    }
   ],
   "source": [
    "for m in mods:\n",
    "    print(lsuv_module(m, xb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fastai course first fixes the biases and then the weights. But in that way fixing the weights ruins the bias more than in this order!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's train again and compare to the baseline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.4337325390625, tensor(0.8606, device='cuda:0')]\n",
      "valid: [0.1249064697265625, tensor(0.9631, device='cuda:0')]\n",
      "train: [0.12725626953125, tensor(0.9613, device='cuda:0')]\n",
      "valid: [0.092294921875, tensor(0.9713, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a better loss and accuracy and end with a better loss and accuracy compared to the training without lsuv.\n",
    "\n",
    "Lsuv is especially useful for more complex and deeper architectures that are hard to initialize to get unit variance at the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 07a_lsuv.ipynb to exp/nb_07a.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 07a_lsuv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastaiV1",
   "language": "python",
   "name": "fastaiv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
